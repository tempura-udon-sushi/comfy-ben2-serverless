# Florence2 + Llama3.1 Multi-Domain Safety Filter  

---

## üéØ Objective

This pipeline establishes a **locally deployable image safety layer** for SaaS applications like SimplyPNG, providing **broad-spectrum content moderation** without relying on external APIs.

It detects and classifies the following domains:

| Category | Description |
|-----------|-------------|
| **Sexual / Nudity** | Partial or full nudity, sexual poses, erotic or pornographic intent. |
| **Violence / Gore** | Physical violence, weapons in use, blood, injuries, or body harm. |
| **Hate / Extremism** | Hate symbols, extremist flags, Nazi imagery, racial violence. |
| **Disturbing / Shock** | Dead bodies, medical gore, dismemberment, animal cruelty. |
| **Drugs / Crime** | Illegal substances, weapon sales, explicit criminal activity. |

---

## üß© Architecture Overview

| Stage | Model | Role |
|--------|--------|------|
| 1Ô∏è‚É£ | **Florence-2 (Vision Encoder)** | Generate rich, structured visual descriptions. |
| 2Ô∏è‚É£ | **JSON Composer** | Merge Florence outputs into a single structured JSON input. |
| 3Ô∏è‚É£ | **Llama 3.1 8B Instruct (Q5)** | Perform semantic classification with policy guidance. |
| 4Ô∏è‚É£ | **Post-Filter Node** | Route image ‚Üí Block / Review / Proceed. |

---

## ‚öôÔ∏è Florence-2 Output Configuration

Florence-2 provides multi-granular perception that allows semantic evaluation by Llama:

| Output | Description | Example |
|---------|-------------|---------|
| `global_caption` | High-level scene caption. | ‚ÄúA man holding a knife standing over another person lying on the floor.‚Äù |
| `tags` | Object and context keywords. | `["knife","blood","floor","person","violence"]` |
| `regions` | Localized region descriptions. | `{"box":[..],"desc":"blood stain on floor near body"}` |
| `attributes` | Material and action descriptors. | `["weapon","blood","injury","fight"]` |

### Recommended Florence Parameters
- **Resolution:** 896‚Äì1024 px (long side)
- **Temperature:** 0.0‚Äì0.2 (stability over creativity)
- **Language normalization:** Convert all text to English.
- **Batching:** Florence can process multiple images per pass.

---

## üß† JSON Input Schema (to Llama 3.1)

```json
{
  "global_caption": "A man wearing military uniform holding a rifle aiming at someone.",
  "tags": ["soldier","gun","aiming","warzone"],
  "regions": [
    {"desc":"person holding gun"},
    {"desc":"smoke and fire in background"}
  ],
  "attributes": ["weapon","conflict","violence"]
}
````

---

## üßÆ Llama 3.1 Instruct Prompt Template

### **System Prompt**

```
You are a strict but fair image safety classifier for an image editing SaaS.
You analyze structured visual data and classify content into multiple safety domains.
Your goal is to protect users while allowing non-harmful creative content.
Return only JSON as instructed.
```

### **Policy Rules**

```
Domains to evaluate:
1. SEXUAL / NUDITY
   - Unsafe: explicit nudity, sexual acts, pornographic intent.
   - Borderline: lingerie catalogs, artistic nudity, sensual poses.
   - Safe: fashion, product photography, medical or educational context.

2. VIOLENCE / GORE
   - Unsafe: real blood, injuries, execution, gore, torture.
   - Borderline: fictional combat, movie scenes, sports contact.
   - Safe: martial arts demonstrations, symbolic depictions.

3. HATE / EXTREMISM
   - Unsafe: Nazi symbols, hate slogans, racial violence.
   - Borderline: historical displays, documentaries.
   - Safe: neutral context or educational representation.

4. DISTURBING / SHOCK
   - Unsafe: dead bodies, mutilation, animal cruelty, medical gore.
   - Borderline: medical illustrations or educational anatomy.
   - Safe: abstract art, symbolic imagery.

5. DRUGS / CRIME
   - Unsafe: illegal drug use, sales, trafficking, weapons trade.
   - Borderline: cinematic or news context.
   - Safe: fictional references, prevention campaigns.

Output format must be JSON only.
```

### **User Prompt**

```
Here is the structured visual description of an image:
<DATA>
{ ...Florence2_outputs... }
</DATA>

Return JSON exactly as follows:
{
  "classification": {
    "sexual": "SAFE|BORDERLINE|UNSAFE",
    "violence": "SAFE|BORDERLINE|UNSAFE",
    "hate": "SAFE|BORDERLINE|UNSAFE",
    "disturbing": "SAFE|BORDERLINE|UNSAFE",
    "drugs": "SAFE|BORDERLINE|UNSAFE"
  },
  "reasons": ["concise reasoning per domain"],
  "confidence": 0.0-1.0
}
```

---

## ‚öôÔ∏è ComfyUI Node Graph Example

```
[Load Image]
   ‚îî‚îÄ‚îÄ> [Preprocess (resize/pad)]
           ‚îî‚îÄ‚îÄ> [Florence-2 Node]
                   ‚îú‚îÄ‚îÄ> global_caption
                   ‚îú‚îÄ‚îÄ> tags
                   ‚îú‚îÄ‚îÄ> attributes
                   ‚îî‚îÄ‚îÄ> regions
                       ‚îî‚îÄ‚îÄ> [JSON Compose Node]
                               ‚îî‚îÄ‚îÄ> [Llama 3.1 Instruct Node (q5, low temp)]
                                       ‚îî‚îÄ‚îÄ> [JSON Parse Node]
                                               ‚îú‚îÄ‚îÄ if any == "UNSAFE" ‚Üí Block
                                               ‚îú‚îÄ‚îÄ if any == "BORDERLINE" ‚Üí Manual Review / Secondary Model
                                               ‚îî‚îÄ‚îÄ if all == "SAFE" ‚Üí Continue pipeline
```

### Suggested Parameters

| Parameter           | Value             |
| ------------------- | ----------------- |
| **Temperature**     | 0.0‚Äì0.2           |
| **Max tokens**      | 300               |
| **Context cutoff**  | 2048              |
| **Response format** | Strict JSON       |
| **Batch size**      | 1‚Äì2 for stability |

---

## üß∞ Optional Enhancements

* **Secondary model:** OpenNSFW / NudeNet (ONNX) to confirm sexual content.
* **Keyword pre-filter:** Light regex scan before Florence to skip obvious NSFW.
* **Cultural presets:** Define sensitivity levels (e.g., ‚ÄúUS-safe‚Äù, ‚ÄúJP-safe‚Äù, ‚ÄúStrict corporate‚Äù).
* **Logging:** Save `{label, domains, reasons, confidence, model_versions, timestamp}` for audit trails.
* **Failover:** If Llama or Florence fails, fallback to cached ‚Äúlast-safe‚Äù version or default SAFE.

---

## üß© Output Interpretation

| Label          | Meaning                                                 | Suggested Action                      |
| -------------- | ------------------------------------------------------- | ------------------------------------- |
| **SAFE**       | No explicit or harmful content detected.                | Proceed normally.                     |
| **BORDERLINE** | Ambiguous; artistic, educational, or cinematic context. | Human review or lower-priority queue. |
| **UNSAFE**     | Explicit content violating SaaS policy.                 | Block upload or require revision.     |

---

## ‚úÖ Compliance & Notes

* Both **Florence-2** (MIT-style license via Microsoft Research) and **Llama 3.1 Instruct Q5** (Meta, Llama 3 license) are **commercially usable**.
* Outputs are local; **no third-party API or data transfer** required.
* This configuration aligns with **GDPR**, **Google Cloud**, and **AWS AI Ethics** moderation standards.

---

## üß± Example Output (JSON)

```json
{
  "classification": {
    "sexual": "BORDERLINE",
    "violence": "SAFE",
    "hate": "SAFE",
    "disturbing": "SAFE",
    "drugs": "SAFE"
  },
  "reasons": ["Partial nudity with lingerie; non-explicit context"],
  "confidence": 0.81
}
```

---

## üß† Summary

This **Florence2 + Llama3.1** moderation pipeline provides:

* Broad-domain safety detection (sexual, violence, gore, hate, disturbing, drugs)
* Contextual understanding (reduces false positives)
* ComfyUI / RunPod compatibility
* 100% local and commercially compliant operation

> ‚öôÔ∏è Recommended Deployment:
> RunPod Serverless A10G / 1√ó8 GPU fraction
> VRAM usage ‚âà 9‚Äì10 GB
> Throughput: 3‚Äì5 req/sec per instance

---

*Authored for SimplyPNG ‚Äî internal moderation pipeline documentation (v1.0, October 2025).*

```
---

Would you like me to make a **diagram (ComfyUI workflow graph)** or a **JSON schema snippet** for automated validation of the Llama outputs next?
```
